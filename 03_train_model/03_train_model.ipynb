{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Training</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Amazon SageMaker built-in XGBoost algorithm (https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to train a simple binary classification model, using the pre-processed data generated in the previous step by the AWS Glue job. Let's define some variables first.\n",
    "\n",
    "<span style=\"color: red\"> Please replace your initials in the bucket_name variable defined in next cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "\n",
    "# replace [your-initials] according to the bucket name you have defined.\n",
    "bucket_name = 'endtoendml-workshop-[your-initials]'\n",
    "key_prefix = 'data/preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a look at our preprocessed data, which have already been split into training and validation sets by the AWS Glue job. In order to do that, we first download a preprocessed training file from Amazon S3 to the local notebook file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# The name of the file has been set by AWS Glue job in the previous notebook.\n",
    "train_file_name = 'part-00000'\n",
    "train_file_key = '{0}/train/{1}'.format(key_prefix, train_file_name)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket_name).download_file(train_file_key, train_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the file has been download, we can use Pandas to read the CSV and display the first 10 rows. You can immediately notice that categorical features have been one-hot encoded according to the feature engineering actions executed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "df = pandas.read_csv(train_file_name, header=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start training, we need to specify the location of the docker container that will be used for training.\n",
    "Docker Registry paths for Amazon algorithms are specified here: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "\n",
    "By the way, we can use a utility function of the Amazon SageMaker Python SDK to get the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version=\"latest\")\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training, by specifying the input and output settings and the required hyperparameters. You can find the list of the supported hyperparameters for the XGBoost algorithm here: https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html.\n",
    "\n",
    "You can also try running the following cell multiple times changing hyperparameters or other settings like the number of instances to be used for training, since XGBoost can be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "output_location = 's3://{0}/output'.format(bucket_name)\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.2xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    base_job_name='predmain-train-xgb')\n",
    "\n",
    "est.set_hyperparameters(objective='reg:logistic',\n",
    "                        max_depth=4,\n",
    "                        num_round=20)\n",
    "\n",
    "train_config = sagemaker.session.s3_input('s3://{0}/{1}/train/'.format(\n",
    "    bucket_name, key_prefix), content_type='text/csv')\n",
    "val_config = sagemaker.session.s3_input('s3://{0}/{1}/val/'.format(\n",
    "    bucket_name, key_prefix), content_type='text/csv')\n",
    "\n",
    "est.fit({'train': train_config, 'validation': val_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is completed, the serialized model will be saved in the S3 output_location defined above.\n",
    "You can now move to the next notebook in the **04_deploy_model** folder to see how to use that model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
