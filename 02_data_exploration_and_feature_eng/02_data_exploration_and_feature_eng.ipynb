{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data exploration, preprocessing and feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and the following notebooks we will demonstrate how you can build your ML Pipeline leveraging Spark Feature Transformers and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and XGBoost) as a SageMaker Inference Pipeline behind a single Endpoint for real-time inference.\n",
    "\n",
    "In particular, in this notebook we will tackle the first steps related to data exploration and preparation. We will use [Amazon Athena](https://aws.amazon.com/athena/) to query our dataset and have a first insight about data quality and available features, and [AWS Glue](https://aws.amazon.com/glue/) to create a Data Catalog and run serverless Spark jobs.\n",
    "\n",
    "<span style=\"color: red\"><strong>To get started, in the cell below please replace your initials in the bucket_name variable, in order to match the bucket name you've created in the previous steps.</strong></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    " \n",
    "# replace [your-initials] according to the bucket name you have defined.\n",
    "bucket_name = 'endtoendml-workshop-[your-initials]'\n",
    "\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now copy to our bucket the dataset used for this use case. We will use the `windturbine_raw_data.csv` made available for this workshop in the `gianpo-public` public S3 bucket. In this Notebook, we will download from that bucket and upload to your bucket so that AWS Glue can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "file_key = 'data/raw/windturbine_raw_data.csv'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "\n",
    "s3.Bucket(bucket_name).object_versions.delete()\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need now is to infer a schema for our dataset. Thanks to its [integration with AWS Glue](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html), we will later use Amazon Athena to run SQL queries against our data stored in S3 without the need to import them into a relational database. To do so, Amazon Athena uses the AWS Glue Data Catalog as a central location to store and retrieve table metadata throughout an AWS account. The Athena execution engine, indeed, requires table metadata that instructs it where to read data, how to read it, and other information necessary to process the data.\n",
    "\n",
    "To organize our Glue Data Catalog we create a new database named `endtoendml-db`. To do so, we create a Glue client via Boto and invoke the `create_database` method.\n",
    "\n",
    "However, first we want to make sure these AWS resources to not exist yet to avoid any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Trying to remove any existing database, crawler and job with the same name.\n",
    "\n",
    "crawler_found = True\n",
    "try:\n",
    "    glue_client.get_crawler(Name = 'endtoendml-crawler')\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    crawler_found = False\n",
    "\n",
    "db_found = True\n",
    "try:\n",
    "    glue_client.get_database(Name = 'endtoendml-db')\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    db_found = False\n",
    "    \n",
    "job_found = True\n",
    "try:\n",
    "    glue_client.get_job(JobName = 'endtoendml-job')\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    job_found = False\n",
    "\n",
    "if crawler_found:\n",
    "    glue_client.delete_crawler(Name = 'endtoendml-crawler')\n",
    "if db_found:\n",
    "     glue_client.delete_database(Name = 'endtoendml-db')\n",
    "if job_found:\n",
    "     glue_client.delete_job(JobName = 'endtoendml-job')\n",
    "\n",
    "print(\"Cleanup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.create_database(DatabaseInput={'Name': 'endtoendml-db'})\n",
    "response = glue_client.get_database(Name='endtoendml-db')\n",
    "response\n",
    "assert response['Database']['Name'] == 'endtoendml-db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a Glue Crawler that we point to the S3 path where the dataset resides, and the crawler creates table definitions in the Data Catalog.\n",
    "To grant the correct set of access permission to the crawler, we use one of the roles created before (`GlueServiceRole-endtoendml`) whose policy grants AWS Glue access to data stored in your S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.create_crawler(\n",
    "    Name='endtoendml-crawler',\n",
    "    Role='service-role/GlueServiceRole-endtoendml', \n",
    "    DatabaseName='endtoendml-db',\n",
    "    Targets={'S3Targets': [{'Path': '{0}/data/raw/'.format(bucket_name)}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the crawler with the `start_crawler` API and to monitor its status upon completion through the `get_crawler_metrics` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_crawler(Name='endtoendml-crawler')\n",
    "\n",
    "while glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 0:\n",
    "    print('RUNNING')\n",
    "    time.sleep(15)\n",
    "    \n",
    "assert glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the crawler has finished its job, we can retrieve the Table definition for the newly created table.\n",
    "As you can see, the crawler has been able to correctly identify 12 fields, infer a type for each column and assign a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = glue_client.get_table(DatabaseName='endtoendml-db', Name='raw')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge of the dataset, we can assign more specific names to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['Table']['StorageDescriptor']['Columns'] = [{'Name': 'turbine_id', 'Type': 'string'},\n",
    "                                                  {'Name': 'turbine_type', 'Type': 'string'},\n",
    "                                                  {'Name': 'wind_speed', 'Type': 'double'},\n",
    "                                                  {'Name': 'rpm_blade', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_level', 'Type': 'double'},\n",
    "                                                  {'Name': 'temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'humidity', 'Type': 'double'},\n",
    "                                                  {'Name': 'vibrations_frequency', 'Type': 'double'},\n",
    "                                                  {'Name': 'pressure', 'Type': 'double'},\n",
    "                                                  {'Name': 'wind_direction', 'Type': 'string'},\n",
    "                                                  {'Name': 'breakdown', 'Type': 'string'}]\n",
    "updated_table = table['Table']\n",
    "updated_table.pop('DatabaseName', None)\n",
    "updated_table.pop('CreateTime', None)\n",
    "updated_table.pop('UpdateTime', None)\n",
    "updated_table.pop('CreatedBy', None)\n",
    "updated_table.pop('IsRegisteredWithLakeFormation', None)\n",
    "\n",
    "glue_client.update_table(\n",
    "    DatabaseName='endtoendml-db',\n",
    "    TableInput=updated_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration with Amazon Athena</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data exploration, let's install PyAthena, a Python client for Amazon Athena. Note: PyAthena is not maintained by AWS, please visit: https://pypi.org/project/PyAthena/ for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyathena\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "athena_cursor = connect(s3_staging_dir='s3://{0}/staging/'.format(bucket_name), \n",
    "                        region_name=region).cursor()\n",
    "\n",
    "athena_cursor.execute('SELECT * FROM \"endtoendml-db\".raw limit 8;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another SQL query to count how many records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_cursor.execute('SELECT COUNT(*) FROM \"endtoendml-db\".raw;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see what are possible values for the field \"breakdown\" and how frequently they occur over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_cursor.execute('SELECT breakdown, (COUNT(breakdown) * 100.0 / (SELECT COUNT(*) FROM \"endtoendml-db\".raw)) \\\n",
    "            AS percent FROM \"endtoendml-db\".raw GROUP BY breakdown;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_cursor.execute('SELECT breakdown, COUNT(breakdown) AS bd_count FROM \"endtoendml-db\".raw GROUP BY breakdown;')\n",
    "df = pd.read_csv(athena_cursor.output_location)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(df.breakdown, df.bd_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered that the dataset is quite unbalanced, although we are not going to try balancing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_cursor.execute('SELECT DISTINCT(turbine_type) FROM \"endtoendml-db\".raw')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_cursor.execute('SELECT COUNT(*) FROM \"endtoendml-db\".raw WHERE oil_temperature IS NULL GROUP BY oil_temperature')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also realized there are a few null values that need to be managed during the data preparation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of keeping the data exploration step short during the workshop, we are not going to execute additional queries. However, feel free to explore the dataset more if you have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you can go to Amazon Athena console and check for query duration under History tab: usually queries are executed in a few seconds, then it some time for Pandas to load results into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing and Feature Engineering with AWS Glue</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing and feature engineering code is implemented in the endtoendml_etl.py file. You can go through the code and see that several categorical columns required indexing and one-hot encoding.\n",
    "Once the Spark ML Pipeline fit() and transform() is done, we are splitting our dataset into 80-20 train & validation as part of the script and uploading to S3 so that it can be used with XGBoost for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize endtoendml_etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our job, we will also have to pass MLeap dependencies to Glue. MLeap is an additional library we are using which does not come bundled with default Spark. (https://github.com/combust/mleap). MLeap is used here to serialize the SparkML model and create a bundle that will be used later for inference.\n",
    "\n",
    "Similar to most of the packages in the Spark ecosystem, MLeap is also implemented as a Scala package with a front-end wrapper written in Python so that it can be used from PySpark. We need to make sure that the MLeap Python library as well as the JAR is available within the Glue job environment. In the following cell, we will download the MLeap Python dependency & JAR from a SageMaker hosted bucket and upload to the S3 bucket we created above in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "!wget -nc https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(bucket_name).upload_file('python.zip', 'dependencies/python/python.zip')\n",
    "s3.Bucket(bucket_name).upload_file('mleap_spark_assembly.jar', 'dependencies/jar/mleap_spark_assembly.jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be uploading the endtoendml_etl.py script to S3 now so that Glue can use it to run the PySpark job. Then, we create the AWS Glue job definition, which includes the S3 paths for the extra dependencies defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(bucket_name).upload_file('endtoendml_etl.py', 'code/endtoendml_etl.py')\n",
    "\n",
    "ETLJob = glue_client.create_job(Name='endtoendml-job', \n",
    "                                Role='GlueServiceRole-endtoendml',\n",
    "                                Command={\n",
    "                                    'Name': 'glueetl',\n",
    "                                    'ScriptLocation': 's3://{0}/code/endtoendml_etl.py'.format(bucket_name)\n",
    "                                },\n",
    "                               DefaultArguments={\n",
    "                                   '--job-language': 'python',\n",
    "                                   '--extra-jars' : 's3://{0}/dependencies/jar/mleap_spark_assembly.jar'.format(bucket_name),\n",
    "                                   '--extra-py-files': 's3://{0}/dependencies/python/python.zip'.format(bucket_name),\n",
    "                                   '--enable-continuous-cloudwatch-log': 'true',\n",
    "                                   '--enable-continuous-log-filter': 'true'\n",
    "                               })\n",
    "glue_job_name = ETLJob['Name']\n",
    "print(glue_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can **start** our preprocessing and feature engineering job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\"><strong>Note: running the job has been commented out since it requires around 15 minutes to be executed. For the purpose of saving time executing this workshop, the outputs of the job (preprocessed files and Feature Transformer SparkML model) are provided in a S3 bucket and can be copied to the expected target locations, just by executing the code below.</strong></div>\n",
    "<div style=\"color: red\">Alternatively, you can skip running next cell and uncomment the code in the subsequent cells, in order to run the job from scratch.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "file_key = 'data/preprocessed/train/part-00000'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)\n",
    "\n",
    "file_key = 'data/preprocessed/val/part-00000'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)\n",
    "\n",
    "file_key = 'output/sparkml/model.tar.gz'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JobRun = glue_client.start_job_run(JobName=glue_job_name, \n",
    "#                                  Arguments = {'--S3_BUCKET': bucket_name})\n",
    "#print(JobRun)\n",
    "\n",
    "# Running the job will take around 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#status = glue_client.get_job_run(JobName=ETLJob['Name'], RunId=JobRun['JobRunId'])\n",
    "#while status['JobRun']['JobRunState'] not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "#    print('Job status: ' + status['JobRun']['JobRunState'])\n",
    "#    time.sleep(30)\n",
    "#    status = glue_client.get_job_run(JobName=ETLJob['Name'], RunId=JobRun['JobRunId'])\n",
    "\n",
    "#print(status['JobRun']['JobRunState'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing and feature engineering are completed, you can move to the next notebook in the **03_train_model** folder to start model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
