{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pipeline Model Deployment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built and trained our models for feature engineering (using AWS Glue and SparkML) and binary classification (using the XGBoost built-in algorithm in Amazon SageMaker), we can choose to deploy them in a pipeline using Amazon SageMaker Inference Pipelines.\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html\n",
    "\n",
    "This notebook demonstrates how to create a pipeline with the SparkML model for feature engineering and the Amazon SageMaker XGBoost model for binary classification.\n",
    "\n",
    "<span style=\"color: red\"> Please replace your initials in the bucket_name variable defined in next cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "\n",
    "# replace [your-initials] according to the bucket name you have defined.\n",
    "bucket_name = 'endtoendml-workshop-[your-initials]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create two Amazon SageMaker **Model** objects, which combine the artifacts of training (serialized model artifacts in Amazon S3) and the Docker container used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that, we need to get the paths to our serialized models in S3.\n",
    "<ul>\n",
    "    <li>For the SparkML model, we defined the path where the artifacts have been stored in step 02</li>\n",
    "    <li>For the XGBoost model, we need to find the path based on Amazon SageMaker's naming convention</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_latest_training_job_name(base_job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.list_training_jobs(NameContains=base_job_name, SortBy='CreationTime', \n",
    "                                         SortOrder='Descending', StatusEquals='Completed')\n",
    "    if len(response['TrainingJobSummaries']) > 0 :\n",
    "        return response['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    else:\n",
    "        raise Exception('Training job not found.')\n",
    "\n",
    "def get_training_job_s3_model_artifacts(job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.describe_training_job(TrainingJobName=job_name)\n",
    "    s3_model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    return s3_model_artifacts\n",
    "\n",
    "# SparkML model path.\n",
    "sparkml_model_path = 's3://{0}/output/sparkml/model.tar.gz'.format(bucket_name)\n",
    "\n",
    "training_base_job_name = 'predmain-train-xgb'\n",
    "latest_training_job_name = get_latest_training_job_name(training_base_job_name)\n",
    "# XGBoost model path.\n",
    "xgboost_model_path = get_training_job_s3_model_artifacts(latest_training_job_name)\n",
    "\n",
    "print('SparkML model path: ' + sparkml_model_path)\n",
    "print('XGBoost model path: ' + xgboost_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to find the ECR (Elastic Container Registry) path of the XGBoost Docker containers that will be used for hosting (actually, this is the same container we used for training).\n",
    "\n",
    "For SparkML serving, the container is provided by AWS so we do not need to worry of finding the ECR path since it will be looked up automatically by the Amazon SageMaker Python SDK.\n",
    "For more info, please see: https://github.com/aws/sagemaker-sparkml-serving-container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "xgboost_container = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version=\"latest\")\n",
    "print(xgboost_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkML serving container needs to know the schema of the request that'll be passed to it while calling the predict method. In order to alleviate the pain of not having to pass the schema with every request, _sagemaker-sparkml-serving_ allows you to pass it via an environment variable while creating the model definitions. This schema definition will be required in our next step for creating a model.\n",
    "\n",
    "You can overwrite this schema on a per request basis by passing it as part of the individual request payload as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"turbine_id\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"turbine_type\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"wind_speed\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"rpm_blade\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"oil_temperature\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"oil_level\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"temperature\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"humidity\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"vibrations_frequency\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"pressure\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"wind_direction\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create our **Model** objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "\n",
    "sparkml_preprocessor_model = SparkMLModel(model_data=sparkml_model_path, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json}, \n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "print(xgboost_model_path)\n",
    "xgboost_model = Model(xgboost_model_path, xgboost_container, sagemaker_session=sagemaker_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have models ready, we can deploy them in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "pipeline_model_name = 'pred-main-sparkml-xgb-pipeline-{0}'.format(str(int(time.time())))\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=pipeline_model_name, \n",
    "    role=role,\n",
    "    models=[\n",
    "        sparkml_preprocessor_model, \n",
    "        xgboost_model],\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "endpoint_name = 'pred-main-pipeline-endpoint-{0}'.format(str(int(time.time())))\n",
    "print(endpoint_name)\n",
    "\n",
    "pipeline_model.deploy(initial_instance_count=1, \n",
    "                      instance_type='ml.c5.xlarge', \n",
    "                      endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-weight:bold\">Please take note of the endpoint name, since it will be used in the next workshop module.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting inferences</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try invoking our pipeline of models and try getting some inferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=csv_serializer,\n",
    "    content_type=CONTENT_TYPE_CSV,\n",
    "    accept=CONTENT_TYPE_JSON)\n",
    "\n",
    "payload = \"TID008,HAWT,64,80,46,21,55,55,7,34,SE\"\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tested the endpoint, we can move to the next workshop module. Please access the module <a href=\"https://github.com/giuseppeporcelli/end-to-end-ml-application/tree/master/05_API_Gateway_and_Lambda\" target=\"_blank\">05_API_Gateway_and_Lambda</a> on GitHub to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
